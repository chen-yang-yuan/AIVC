{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d94103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenyang/miniconda3/envs/AIVC-env/lib/python3.10/site-packages/louvain/__init__.py:54: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n",
      "/Users/chenyang/miniconda3/envs/AIVC-env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_csv from `anndata` is deprecated. Import anndata.io.read_csv instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/chenyang/miniconda3/envs/AIVC-env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_excel from `anndata` is deprecated. Import anndata.io.read_excel instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/chenyang/miniconda3/envs/AIVC-env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_hdf from `anndata` is deprecated. Import anndata.io.read_hdf instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/chenyang/miniconda3/envs/AIVC-env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_loom from `anndata` is deprecated. Import anndata.io.read_loom instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/chenyang/miniconda3/envs/AIVC-env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_mtx from `anndata` is deprecated. Import anndata.io.read_mtx instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/chenyang/miniconda3/envs/AIVC-env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/chenyang/miniconda3/envs/AIVC-env/lib/python3.10/site-packages/anndata/utils.py:434: FutureWarning: Importing read_umi_tools from `anndata` is deprecated. Import anndata.io.read_umi_tools instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/chenyang/miniconda3/envs/AIVC-env/lib/python3.10/site-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/Users/chenyang/miniconda3/envs/AIVC-env/lib/python3.10/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/Users/chenyang/miniconda3/envs/AIVC-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import scanpy as sc\n",
    "import scgpt.tasks.cell_emb as cell_emb\n",
    "import torch\n",
    "\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.tasks.cell_emb import get_batch_cell_embeddings\n",
    "from scgpt.tokenizer import GeneVocab\n",
    "from scgpt.utils import load_pretrained\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sc.settings.verbosity = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c8a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable multiprocessing\n",
    "if not hasattr(os, \"sched_getaffinity\"):\n",
    "    os.sched_getaffinity = lambda x: [0]\n",
    "    \n",
    "_Orig = DataLoader\n",
    "def NoMP(*args, **kwargs):\n",
    "    kwargs[\"num_workers\"] = 0\n",
    "    return _Orig(*args, **kwargs)\n",
    "\n",
    "torch.utils.data.DataLoader = NoMP\n",
    "cell_emb.DataLoader = NoMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9eb78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and parameters\n",
    "data_path = \"../../data/merged_data/\"\n",
    "model_dir = \"../../data/_utils/scGPT/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eaca138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 51200 × 5001\n",
       "    obs: 'cell_id', 'global_x', 'global_y', 'transcript_counts', 'control_probe_counts', 'genomic_control_counts', 'control_codeword_counts', 'unassigned_codeword_counts', 'deprecated_codeword_counts', 'total_counts', 'cell_area', 'nucleus_area', 'nucleus_count', 'segmentation_method', 'cell_type_merged', 'batch'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'gene'\n",
       "    uns: 'batch_colors', 'cell_type_merged_colors'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and subset data\n",
    "adata = sc.read_h5ad(data_path + \"adata_all_raw.h5ad\")\n",
    "np.random.seed(42)\n",
    "idx = np.random.choice(adata.n_obs, 51200, replace=False)\n",
    "adata = adata[idx].copy()\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea32178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "vocab_file = os.path.join(model_dir, \"vocab.json\")\n",
    "vocab = GeneVocab.from_file(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5749eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map genes to vocabulary indices\n",
    "adata.var[\"id_in_vocab\"] = [vocab[g] if g in vocab else -1 for g in adata.var[\"gene\"]]\n",
    "adata = adata[:, adata.var[\"id_in_vocab\"] >= 0].copy()\n",
    "gene_ids = np.array(adata.var[\"id_in_vocab\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa31b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model configurations\n",
    "with open(os.path.join(model_dir, \"args.json\"), \"r\") as f:\n",
    "    model_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18035e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = TransformerModel(\n",
    "    ntoken=len(vocab),\n",
    "    d_model=model_configs[\"embsize\"],\n",
    "    nhead=model_configs[\"nheads\"],\n",
    "    d_hid=model_configs[\"d_hid\"],\n",
    "    nlayers=model_configs[\"nlayers\"],\n",
    "    nlayers_cls=model_configs[\"n_layers_cls\"],\n",
    "    vocab=vocab,\n",
    "    dropout=model_configs[\"dropout\"],\n",
    "    pad_token=model_configs[\"pad_token\"],\n",
    "    pad_value=model_configs[\"pad_value\"],\n",
    "    do_mvc=True,\n",
    "    do_dab=False,\n",
    "    use_batch_labels=False,\n",
    "    domain_spec_batchnorm=False,\n",
    "    explicit_zero_prob=True,\n",
    "    n_input_bins=model_configs[\"n_bins\"],\n",
    "    use_fast_transformer=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c27522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading parameter encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading parameter value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading parameter decoder.fc.4.bias with shape torch.Size([1])\n",
      "scGPT - INFO - Loading parameter mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter mvc_decoder.gene2query.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter mvc_decoder.W.weight with shape torch.Size([512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ExprDecoder(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (zero_logit): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cls_decoder): ClsDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (mvc_decoder): MVCDecoder(\n",
       "    (gene2query): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (query_activation): Sigmoid()\n",
       "    (W): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (W_zero_logit): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (sim): Similarity(\n",
       "    (cos): CosineSimilarity()\n",
       "  )\n",
       "  (creterion_cce): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model weights\n",
    "state_dict = torch.load(os.path.join(model_dir, \"best_model.pt\"), map_location=\"cpu\")\n",
    "load_pretrained(model, state_dict, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2558a5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 51176/51200 cells with >0 nonzero genes in adata.X\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 51176 × 4993\n",
       "    obs: 'cell_id', 'global_x', 'global_y', 'transcript_counts', 'control_probe_counts', 'genomic_control_counts', 'control_codeword_counts', 'unassigned_codeword_counts', 'deprecated_codeword_counts', 'total_counts', 'cell_area', 'nucleus_area', 'nucleus_count', 'segmentation_method', 'cell_type_merged', 'batch'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'gene', 'id_in_vocab'\n",
       "    uns: 'batch_colors', 'cell_type_merged_colors'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "def filter_empty_cells(adata):\n",
    "    X = adata.X\n",
    "    if sp.issparse(X):\n",
    "        nnz = np.asarray((X != 0).sum(axis=1)).ravel()\n",
    "    else:\n",
    "        nnz = (X != 0).sum(axis=1)\n",
    "    keep = nnz > 0\n",
    "    print(f\"Keeping {keep.sum()}/{adata.n_obs} cells with >0 nonzero genes in adata.X\")\n",
    "    return adata[keep].copy()\n",
    "\n",
    "adata = filter_empty_cells(adata)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3753deda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding cells: 100%|██████████| 800/800 [3:23:18<00:00, 15.25s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Get cell embeddings\n",
    "model.eval()\n",
    "embeddings = get_batch_cell_embeddings(\n",
    "    adata,\n",
    "    model=model,\n",
    "    vocab=vocab,\n",
    "    model_configs=model_configs,\n",
    "    gene_ids=gene_ids,\n",
    "    batch_size=64,\n",
    "    max_length=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43581b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute t-SNE on the embeddings\n",
    "adata.obsm[\"X_scGPT\"] = embeddings\n",
    "X = adata.obsm[\"X_scGPT\"]\n",
    "X_tsne = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\", init=\"pca\").fit_transform(X)\n",
    "adata.obsm[\"X_tsne_scGPT\"] = X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73b3af57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51176, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c682bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE colored by cell_type_merged\n",
    "cell_types = adata.obs[\"cell_type_merged\"].astype(\"category\")\n",
    "colors = cell_types.cat.codes\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "scatter = plt.scatter(X_tsne[:,0], X_tsne[:,1], s=0.5, alpha=0.7, c=colors, cmap=\"tab20\")\n",
    "plt.title(\" \")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "handles = [\n",
    "    plt.Line2D([], [], marker=\"o\", linestyle=\"\", color=scatter.cmap(scatter.norm(i)), label=cat)\n",
    "    for i, cat in enumerate(cell_types.cat.categories)\n",
    "]\n",
    "plt.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Cell Types\")\n",
    "plt.savefig(\"scGPT_tsne_cell_type_merged.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08a30b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE colored by batch\n",
    "batches = adata.obs[\"batch\"].astype(\"category\")\n",
    "colors = batches.cat.codes\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "scatter = plt.scatter(X_tsne[:,0], X_tsne[:,1], s=0.5, alpha=0.7, c=colors, cmap=\"tab10\")\n",
    "plt.title(\" \")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "handles = [\n",
    "    plt.Line2D([], [], marker=\"o\", linestyle=\"\", color=scatter.cmap(scatter.norm(i)), label=cat)\n",
    "    for i, cat in enumerate(batches.cat.categories)\n",
    "]\n",
    "plt.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Batches\")\n",
    "plt.savefig(\"scGPT_tsne_batch.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef988b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIVC-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
